# GPU Provider Setup Guide for Audio Transcription Pipeline

## Overview

This guide provides working setups for running the GPU-accelerated audio transcription pipeline with faster-whisper and pyannote on various GPU cloud providers. After encountering dependency conflicts in Google Colab, these alternatives offer more control and better compatibility.

## Recommended GPU Providers

### 1. Vast.ai (Recommended - Most Flexible)

**Pros:**
- Wide GPU selection (RTX 3090, 4090, A100, etc.)
- Docker container support
- Hourly pricing (as low as $0.20/hr for RTX 3090)
- Full root access
- Persistent storage options

**Setup Instructions:**

```bash
# 1. Choose a machine with:
#    - CUDA 12.0+
#    - Ubuntu 22.04
#    - At least 24GB VRAM (RTX 3090/4090 or better)
#    - PyTorch 2.0+ image preferred

# 2. Connect via SSH or Jupyter

# 3. Setup environment
apt-get update && apt-get install -y ffmpeg git

# 4. Create conda environment (recommended)
conda create -n transcribe python=3.10 -y
conda activate transcribe

# 5. Install dependencies (WORKING COMBINATION)
pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118
pip install faster-whisper==1.0.0
pip install pyannote.audio==3.1.1
pip install python-dotenv psutil gputil

# 6. Clone your code
git clone <your-repo>
cd audio-transcription-pipeline
```

### 2. RunPod.io (User-Friendly)

**Pros:**
- Pre-configured ML templates
- Serverless and dedicated options
- Good for batch processing
- Supports custom Docker images

**Setup Script:**

```bash
#!/bin/bash
# runpod_setup.sh

# Use RunPod PyTorch template
# Select: PyTorch 2.1.0, CUDA 11.8, Python 3.10

# Install in Jupyter terminal or SSH
pip install --upgrade pip
pip install numpy==1.24.3  # Critical for compatibility
pip install faster-whisper==1.0.0
pip install pyannote.audio==3.1.1
pip install python-dotenv

# Download models in advance (speeds up processing)
python -c "from pyannote.audio import Pipeline; Pipeline.from_pretrained('pyannote/speaker-diarization-3.1', use_auth_token='YOUR_HF_TOKEN')"
```

### 3. Lambda Labs (Professional)

**Pros:**
- High-end GPUs (A100, H100)
- Pre-installed ML frameworks
- Reliable and fast
- Good documentation

**Setup:**

```bash
# Lambda Labs typically has PyTorch pre-installed
# Connect via SSH

# Create virtual environment
python3 -m venv transcribe_env
source transcribe_env/bin/activate

# Install specific versions
pip install torch==2.1.0+cu118 torchvision==0.16.0+cu118 torchaudio==2.1.0+cu118 -f https://download.pytorch.org/whl/torch_stable.html
pip install faster-whisper==1.0.0
pip install pyannote.audio==3.1.1
```

### 4. Paperspace Gradient (Notebook-Friendly)

**Pros:**
- Jupyter-first interface
- Free tier available
- Persistent notebooks
- Good for experimentation

**Setup in Gradient Notebook:**

```python
# Cell 1: Environment setup
!pip uninstall -y torch torchvision torchaudio
!pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118
!pip install faster-whisper==1.0.0
!pip install pyannote.audio==3.1.1

# Cell 2: Restart kernel after installation
import os
os._exit(00)

# Cell 3: Test imports
import torch
from faster_whisper import WhisperModel
from pyannote.audio import Pipeline
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU: {torch.cuda.get_device_name(0)}")
```

## Complete Working Pipeline

Save this as `transcribe_gpu.py`:

```python
#!/usr/bin/env python3
"""
GPU-Accelerated Transcription Pipeline
Works on Vast.ai, RunPod, Lambda Labs, etc.
"""

import os
import sys
import json
import torch
import argparse
from datetime import datetime
from faster_whisper import WhisperModel
from pyannote.audio import Pipeline

def setup_environment():
    """Verify GPU and set optimal settings"""
    if not torch.cuda.is_available():
        print("‚ùå No GPU detected!")
        sys.exit(1)

    # Set optimal GPU settings
    torch.backends.cudnn.benchmark = True
    torch.backends.cuda.matmul.allow_tf32 = True

    device = torch.device("cuda")
    gpu_name = torch.cuda.get_device_name(0)
    vram = torch.cuda.get_device_properties(0).total_memory / 1024**3

    print(f"‚úì GPU: {gpu_name}")
    print(f"‚úì VRAM: {vram:.1f} GB")

    # Check HuggingFace token
    hf_token = os.environ.get('HF_TOKEN', os.environ.get('HUGGINGFACE_TOKEN'))
    if not hf_token:
        print("‚ö† Warning: No HuggingFace token found. Diarization may fail.")
        print("  Set: export HF_TOKEN='your_token_here'")

    return device, hf_token

def transcribe_audio(audio_file, model_size="large-v3", device="cuda"):
    """Transcribe audio using faster-whisper"""
    print(f"\nüìù Transcribing: {audio_file}")

    # Load model with optimal settings for GPU
    compute_type = "float16" if "A100" in torch.cuda.get_device_name(0) else "int8"

    model = WhisperModel(
        model_size,
        device=device,
        compute_type=compute_type,
        num_workers=4,
        download_root="/tmp/whisper-models"
    )

    # Transcribe with VAD
    segments, info = model.transcribe(
        audio_file,
        beam_size=5,
        language="en",
        vad_filter=True,
        vad_parameters={
            "threshold": 0.5,
            "min_speech_duration_ms": 250,
            "min_silence_duration_ms": 500,
            "speech_pad_ms": 400,
        },
        temperature=0,
        compression_ratio_threshold=2.4,
        log_prob_threshold=-1.0,
        no_speech_threshold=0.6
    )

    # Convert generator to list
    segments_list = []
    for segment in segments:
        segments_list.append({
            "start": segment.start,
            "end": segment.end,
            "text": segment.text.strip()
        })

    print(f"‚úì Transcribed {len(segments_list)} segments")
    print(f"  Duration: {info.duration:.1f}s")

    return segments_list, info

def diarize_audio(audio_file, num_speakers=2, hf_token=None):
    """Run speaker diarization using pyannote"""
    if not hf_token:
        print("‚ö† Skipping diarization (no HF token)")
        return None

    print(f"\nüé≠ Running speaker diarization...")

    try:
        # Load pipeline
        pipeline = Pipeline.from_pretrained(
            "pyannote/speaker-diarization-3.1",
            use_auth_token=hf_token
        )

        # Move to GPU
        pipeline.to(torch.device("cuda"))

        # Run diarization
        diarization = pipeline(audio_file, num_speakers=num_speakers)

        # Extract speaker segments
        speaker_segments = []
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            speaker_segments.append({
                "start": turn.start,
                "end": turn.end,
                "speaker": speaker
            })

        print(f"‚úì Found {len(speaker_segments)} speaker turns")
        return speaker_segments

    except Exception as e:
        print(f"‚ùå Diarization failed: {e}")
        return None

def align_speakers(transcription_segments, speaker_segments):
    """Align transcription with speaker labels"""
    if not speaker_segments:
        # Fallback: simple alternating speakers
        aligned = []
        current_speaker = "SPEAKER_00"
        last_end = 0

        for seg in transcription_segments:
            pause = seg['start'] - last_end
            if pause > 2.0:  # Switch on long pauses
                current_speaker = "SPEAKER_01" if current_speaker == "SPEAKER_00" else "SPEAKER_00"

            aligned.append({
                "start": seg['start'],
                "end": seg['end'],
                "text": seg['text'],
                "speaker": current_speaker
            })
            last_end = seg['end']

        return aligned

    # Proper alignment with diarization
    aligned = []
    for trans_seg in transcription_segments:
        mid_point = (trans_seg['start'] + trans_seg['end']) / 2

        # Find best matching speaker segment
        best_speaker = "UNKNOWN"
        best_overlap = 0

        for spk_seg in speaker_segments:
            if spk_seg['start'] <= mid_point <= spk_seg['end']:
                overlap = min(trans_seg['end'], spk_seg['end']) - max(trans_seg['start'], spk_seg['start'])
                if overlap > best_overlap:
                    best_overlap = overlap
                    best_speaker = spk_seg['speaker']

        aligned.append({
            "start": trans_seg['start'],
            "end": trans_seg['end'],
            "text": trans_seg['text'],
            "speaker": best_speaker
        })

    return aligned

def identify_roles(segments):
    """Identify Therapist vs Client based on speech patterns"""
    speakers_stats = {}

    for seg in segments:
        speaker = seg['speaker']
        if speaker not in speakers_stats:
            speakers_stats[speaker] = {
                'count': 0,
                'questions': 0,
                'therapeutic': 0
            }

        text = seg['text'].lower()
        speakers_stats[speaker]['count'] += 1

        # Count questions
        if '?' in seg['text'] or any(q in text for q in ['how do', 'what do', 'tell me']):
            speakers_stats[speaker]['questions'] += 1

        # Count therapeutic language
        if any(t in text for t in ['it sounds like', "you're saying", 'i hear', 'that must']):
            speakers_stats[speaker]['therapeutic'] += 1

    # Score and identify
    scores = {}
    for speaker, stats in speakers_stats.items():
        if stats['count'] > 0:
            score = (stats['questions'] / stats['count']) * 100 + stats['therapeutic'] * 50
            scores[speaker] = score

    if scores:
        therapist = max(scores, key=scores.get)
        mapping = {therapist: 'Therapist'}
        for spk in scores:
            if spk != therapist:
                mapping[spk] = 'Client'
        return mapping

    return {}

def process_audio(audio_file, output_dir=".", num_speakers=2, model_size="large-v3"):
    """Main processing pipeline"""
    start_time = datetime.now()

    # Setup
    device, hf_token = setup_environment()

    # Step 1: Transcribe
    transcription, info = transcribe_audio(audio_file, model_size, device)

    # Step 2: Diarize
    speaker_segments = diarize_audio(audio_file, num_speakers, hf_token)

    # Step 3: Align
    aligned = align_speakers(transcription, speaker_segments)

    # Step 4: Identify roles
    role_mapping = identify_roles(aligned)

    # Apply roles
    final_segments = []
    for seg in aligned:
        final_segments.append({
            "start": round(seg['start'], 2),
            "end": round(seg['end'], 2),
            "text": seg['text'],
            "speaker": role_mapping.get(seg['speaker'], seg['speaker'])
        })

    # Calculate statistics
    processing_time = (datetime.now() - start_time).total_seconds()

    # Create output
    output = {
        "audio_file": audio_file,
        "duration": round(info.duration, 2),
        "segments": final_segments,
        "metadata": {
            "total_segments": len(final_segments),
            "processing_time": round(processing_time, 2),
            "processing_speed": round(info.duration / processing_time, 2),
            "model": model_size,
            "diarization": speaker_segments is not None
        }
    }

    # Save output
    output_file = os.path.join(output_dir, "transcription_result.json")
    with open(output_file, 'w') as f:
        json.dump(output, f, indent=2)

    print(f"\n‚úÖ Complete!")
    print(f"  Output: {output_file}")
    print(f"  Segments: {len(final_segments)}")
    print(f"  Time: {processing_time:.1f}s")
    print(f"  Speed: {output['metadata']['processing_speed']:.1f}x real-time")

    return output

def main():
    parser = argparse.ArgumentParser(description="GPU-Accelerated Transcription")
    parser.add_argument("audio_file", help="Path to audio file")
    parser.add_argument("--speakers", type=int, default=2, help="Number of speakers")
    parser.add_argument("--model", default="large-v3", help="Whisper model size")
    parser.add_argument("--output", default=".", help="Output directory")

    args = parser.parse_args()

    if not os.path.exists(args.audio_file):
        print(f"‚ùå File not found: {args.audio_file}")
        sys.exit(1)

    process_audio(args.audio_file, args.output, args.speakers, args.model)

if __name__ == "__main__":
    main()
```

## Docker Container (For Vast.ai/RunPod)

Create `Dockerfile`:

```dockerfile
FROM pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime

# Install system dependencies
RUN apt-get update && apt-get install -y \
    ffmpeg \
    git \
    && rm -rf /var/lib/apt/lists/*

# Install Python packages
RUN pip install --no-cache-dir \
    faster-whisper==1.0.0 \
    pyannote.audio==3.1.1 \
    python-dotenv \
    psutil \
    gputil

# Set working directory
WORKDIR /app

# Copy your code
COPY . /app/

# Download models in advance (optional but recommended)
ENV TRANSFORMERS_CACHE=/app/models
ENV HF_HOME=/app/models

# Entry point
CMD ["python", "transcribe_gpu.py"]
```

Build and run:

```bash
# Build
docker build -t transcribe-gpu .

# Run with GPU
docker run --gpus all -v /path/to/audio:/data transcribe-gpu python transcribe_gpu.py /data/audio.mp3
```

## Troubleshooting

### Common Issues and Solutions

1. **CUDA Out of Memory**
   ```python
   # Reduce batch size or use smaller model
   model = WhisperModel("medium", device="cuda", compute_type="int8")
   ```

2. **PyAnnote Import Error**
   ```bash
   # Install specific versions
   pip install torchaudio==2.1.0
   pip install pyannote.audio==3.1.1 --no-deps
   pip install -r pyannote_requirements.txt
   ```

3. **NumPy Recursion Error**
   ```bash
   # Downgrade numpy
   pip install numpy==1.24.3
   ```

4. **Slow Processing**
   ```python
   # Enable TF32 for A100/H100 GPUs
   torch.backends.cuda.matmul.allow_tf32 = True
   torch.backends.cudnn.allow_tf32 = True
   ```

## Performance Benchmarks

| GPU | Model | Audio Length | Processing Time | Speed |
|-----|-------|--------------|-----------------|-------|
| RTX 3090 | large-v3 | 60 min | ~4 min | 15x |
| RTX 4090 | large-v3 | 60 min | ~3 min | 20x |
| A100 40GB | large-v3 | 60 min | ~2.5 min | 24x |
| A6000 | large-v3 | 60 min | ~3.5 min | 17x |

## Cost Estimates

| Provider | GPU | $/hour | 1hr Audio Cost | 10hr Audio Cost |
|----------|-----|--------|----------------|-----------------|
| Vast.ai | RTX 3090 | $0.20 | $0.01 | $0.10 |
| Vast.ai | RTX 4090 | $0.40 | $0.02 | $0.20 |
| RunPod | A100 40GB | $1.40 | $0.06 | $0.60 |
| Lambda | A100 40GB | $1.29 | $0.05 | $0.50 |

## Quick Start Commands

```bash
# 1. SSH into your GPU instance

# 2. Clone this repo
git clone <your-repo-url>
cd audio-transcription-pipeline

# 3. Run setup script
chmod +x setup_gpu.sh
./setup_gpu.sh

# 4. Set HuggingFace token
export HF_TOKEN='your_token_here'

# 5. Process audio
python transcribe_gpu.py /path/to/audio.mp3 --speakers 2

# 6. Check results
cat transcription_result.json
```

## Setup Script

Save as `setup_gpu.sh`:

```bash
#!/bin/bash
# Automated setup for GPU providers

echo "üöÄ Setting up GPU transcription environment..."

# Detect Python version
PYTHON_VERSION=$(python3 --version | cut -d' ' -f2 | cut -d'.' -f1,2)
echo "Python version: $PYTHON_VERSION"

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Upgrade pip
pip install --upgrade pip

# Install PyTorch with CUDA
if [[ "$PYTHON_VERSION" == "3.10" ]]; then
    pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118
elif [[ "$PYTHON_VERSION" == "3.11" ]]; then
    pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu121
else
    echo "‚ö† Python $PYTHON_VERSION may have compatibility issues"
    pip install torch torchvision torchaudio
fi

# Install transcription packages
pip install faster-whisper==1.0.0
pip install pyannote.audio==3.1.1
pip install python-dotenv psutil gputil

# Verify installation
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
python -c "from faster_whisper import WhisperModel; print('‚úì faster-whisper installed')"
python -c "from pyannote.audio import Pipeline; print('‚úì pyannote installed')"

echo "‚úÖ Setup complete!"
echo ""
echo "Next steps:"
echo "1. Set your HuggingFace token: export HF_TOKEN='your_token'"
echo "2. Run: python transcribe_gpu.py your_audio.mp3"
```

## Notes

- **Vast.ai** is recommended for best flexibility and cost
- **Python 3.10** with **CUDA 11.8** is the most stable combination
- Always use a virtual environment to avoid conflicts
- Download models in advance to save processing time
- Monitor GPU memory usage with `nvidia-smi`

## Support

For issues specific to GPU providers:
- Vast.ai Discord: https://discord.gg/vast-ai
- RunPod Discord: https://discord.gg/runpod
- Lambda Labs Support: support@lambdalabs.com

Good luck with your GPU-accelerated transcription!